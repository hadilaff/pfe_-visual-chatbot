{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cu101/torch_stable.html\n",
      "Requirement already up-to-date: torch==1.5 in /home/hadil/.local/lib/python3.8/site-packages (1.5.0+cu101)\n",
      "Requirement already up-to-date: torchvision==0.6 in /home/hadil/.local/lib/python3.8/site-packages (0.6.0+cu101)\n",
      "Requirement already satisfied, skipping upgrade: future in /home/hadil/.local/lib/python3.8/site-packages (from torch==1.5) (0.18.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /home/hadil/.local/lib/python3.8/site-packages (from torch==1.5) (1.22.3)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=4.1.1 in /home/hadil/.local/lib/python3.8/site-packages (from torchvision==0.6) (9.1.1)\n",
      "Requirement already satisfied: cython in /home/hadil/.local/lib/python3.8/site-packages (0.29.28)\n",
      "Requirement already satisfied: pyyaml==5.1 in /home/hadil/.local/lib/python3.8/site-packages (5.1)\n",
      "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
      "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-mp38y5cl\n",
      "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-mp38y5cl\n",
      "Requirement already satisfied, skipping upgrade: setuptools>=18.0 in /home/hadil/.local/lib/python3.8/site-packages (from pycocotools==2.0) (62.2.0)\n",
      "Requirement already satisfied, skipping upgrade: cython>=0.27.3 in /home/hadil/.local/lib/python3.8/site-packages (from pycocotools==2.0) (0.29.28)\n",
      "Requirement already satisfied, skipping upgrade: matplotlib>=2.1.0 in /home/hadil/.local/lib/python3.8/site-packages (from pycocotools==2.0) (3.5.2)\n",
      "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.11.0)\n",
      "Requirement already satisfied, skipping upgrade: fonttools>=4.22.0 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (4.33.3)\n",
      "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.22.3)\n",
      "Requirement already satisfied, skipping upgrade: packaging>=20.0 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (21.3)\n",
      "Requirement already satisfied, skipping upgrade: pillow>=6.2.0 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (9.1.1)\n",
      "Requirement already satisfied, skipping upgrade: pyparsing>=2.2.1 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.0.8)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.7 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.8.2)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0) (1.14.0)\n",
      "Building wheels for collected packages: pycocotools\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycocotools: filename=pycocotools-2.0-cp38-cp38-linux_x86_64.whl size=418980 sha256=a162315e344aad70b57e203f3d8445a8bcbf0214aeab9642ef067ec2ccfd0af1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-yw3_iasf/wheels/56/da/49/cb71a7c450b59588934077f431100c05fbde50646ee84a8d40\n",
      "Successfully built pycocotools\n",
      "Installing collected packages: pycocotools\n",
      "  Attempting uninstall: pycocotools\n",
      "    Found existing installation: pycocotools 2.0\n",
      "    Uninstalling pycocotools-2.0:\n",
      "      Successfully uninstalled pycocotools-2.0\n",
      "Successfully installed pycocotools-2.0\n",
      "1.5.0+cu101 False\n",
      "gcc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n",
      "Copyright (C) 2019 Free Software Foundation, Inc.\n",
      "This is free software; see the source for copying conditions.  There is NO\n",
      "warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -U torch==1.5 torchvision==0.6 -f https://download.pytorch.org/whl/cu101/torch_stable.html \n",
    "!pip install cython pyyaml==5.1\n",
    "!pip install -U 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'\n",
    "import torch, torchvision\n",
    "print(torch.__version__, torch.cuda.is_available())\n",
    "!gcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50,preprocess_input\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n",
      "Requirement already satisfied: detectron2==0.1.3 in /home/hadil/.local/lib/python3.8/site-packages (0.1.3+cu101)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (1.1.0)\n",
      "Requirement already satisfied: Pillow in /usr/lib/python3/dist-packages (from detectron2==0.1.3) (7.0.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (0.8.9)\n",
      "Requirement already satisfied: cloudpickle in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (2.0.0)\n",
      "Requirement already satisfied: matplotlib in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (3.5.2)\n",
      "Requirement already satisfied: mock in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (4.0.3)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (4.64.0)\n",
      "Requirement already satisfied: tensorboard in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (2.9.0)\n",
      "Requirement already satisfied: fvcore>=0.1.1 in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (0.1.5.post20220504)\n",
      "Requirement already satisfied: future in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (0.18.2)\n",
      "Requirement already satisfied: pydot in /home/hadil/.local/lib/python3.8/site-packages (from detectron2==0.1.3) (1.4.2)\n",
      "Requirement already satisfied: PyYAML in /home/hadil/.local/lib/python3.8/site-packages (from yacs>=0.1.6->detectron2==0.1.3) (5.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib->detectron2==0.1.3) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib->detectron2==0.1.3) (4.33.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib->detectron2==0.1.3) (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib->detectron2==0.1.3) (1.22.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib->detectron2==0.1.3) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib->detectron2==0.1.3) (3.0.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/hadil/.local/lib/python3.8/site-packages (from matplotlib->detectron2==0.1.3) (2.8.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (1.0.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (1.46.0)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (2.6.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (3.3.6)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (3.20.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/dist-packages (from tensorboard->detectron2==0.1.3) (2.27.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (62.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (1.8.1)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (2.1.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/hadil/.local/lib/python3.8/site-packages (from tensorboard->detectron2==0.1.3) (0.37.1)\n",
      "Requirement already satisfied: iopath>=0.1.7 in /home/hadil/.local/lib/python3.8/site-packages (from fvcore>=0.1.1->detectron2==0.1.3) (0.1.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib->detectron2==0.1.3) (1.14.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/hadil/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.1.3) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/hadil/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.1.3) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /home/hadil/.local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.1.3) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/hadil/.local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.3) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4; python_version < \"3.10\" in /home/hadil/.local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard->detectron2==0.1.3) (4.11.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0; python_version >= \"3\" in /usr/local/lib/python3.8/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5; python_version >= \"3\" in /usr/lib/python3/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.1.3) (2.8)\n",
      "Requirement already satisfied: portalocker in /home/hadil/.local/lib/python3.8/site-packages (from iopath>=0.1.7->fvcore>=0.1.1->detectron2==0.1.3) (2.4.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/hadil/.local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.1.3) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2==0.1.3) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/hadil/.local/lib/python3.8/site-packages (from importlib-metadata>=4.4; python_version < \"3.10\"->markdown>=2.6.8->tensorboard->detectron2==0.1.3) (3.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install detectron2==0.1.3 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/index.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup detectron2 logger\n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# import some common libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "#from google.colab.patches import cv2_imshow\n",
    "\n",
    "# import some common detectron2 utilities\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#liée la BD\n",
    "import psycopg2\n",
    "DB_Host='localhost'\n",
    "DB_Name='chatbot'\n",
    "DB_User='postgres'\n",
    "DB_pass='hadil24'\n",
    "connection = psycopg2.connect(dbname=DB_Name,user=DB_User,password=DB_pass,host=DB_Host)\n",
    "cursor=connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2.data.datasets import register_coco_instances\n",
    "#register_coco_instances(\"deepfashion_train\", {}, \"/content/DeepFashion2/deepfashion2_train.json\", \"/content/DeepFashion2/train/image\")\n",
    "register_coco_instances(\"deepfashion_val\", {}, \"/home/hadil/final/deepfashion2_validation.json\", \"/home/hadil/final/validation/image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg() #la configuration par défaut de détéctron\n",
    "cfg.MODEL.DEVICE='cpu'\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")) #builtin config file.\n",
    "cfg.DATASETS.TRAIN = (\"deepfashion_val\",)\n",
    "cfg.DATASETS.TEST = ()\n",
    "\n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-Detection/faster_rcnn_R_101_FPN_3x.yaml\")  # Let training initialize from model zoo \n",
    "                                                                                                  #pre-trained weights\n",
    "                                                                                                  #URL to the model trained using the given config\n",
    "\n",
    "\"\"\"_BASE_: \"../Base-RCNN-FPN.yaml\"\n",
    "MODEL:\n",
    "  WEIGHTS: \"detectron2://ImageNetPretrained/MSRA/R-101.pkl\"\n",
    "  MASK_ON: False\n",
    "  RESNETS:\n",
    "    DEPTH: 101\n",
    "SOLVER:\n",
    "  STEPS: (210000, 250000)\n",
    "  MAX_ITER: 270000\n",
    "  \"\"\"                                                                                                  \n",
    "\n",
    "cfg.SOLVER.IMS_PER_BATCH = 4\n",
    "cfg.SOLVER.BASE_LR = 0.001\n",
    "cfg.SOLVER.WARMUP_ITERS = 1000\n",
    "cfg.SOLVER.MAX_ITER = 1500  #nombre d'itération\n",
    "cfg.SOLVER.STEPS = (1000, 1500)\n",
    "cfg.SOLVER.GAMMA = 0.05\n",
    "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 13 #nombre de classe\n",
    "\n",
    "cfg.TEST.EVAL_PERIOD = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[06/03 05:19:41 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (6): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (7): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (8): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (9): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (10): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (11): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (12): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (13): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (14): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (15): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (16): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (17): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (18): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (19): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (20): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (21): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (22): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=14, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=52, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[06/03 05:19:52 d2.data.datasets.coco]: \u001b[0mLoading /home/hadil/final/deepfashion2_validation.json takes 10.68 seconds.\n",
      "\u001b[32m[06/03 05:19:52 d2.data.datasets.coco]: \u001b[0mLoaded 32153 images in COCO format from /home/hadil/final/deepfashion2_validation.json\n",
      "\u001b[32m[06/03 05:19:57 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 32153 images left.\n",
      "\u001b[32m[06/03 05:19:58 d2.data.build]: \u001b[0mDistribution of instances among all 13 categories:\n",
      "\u001b[36m|   category    | #instances   |   category    | #instances   |   category    | #instances   |\n",
      "|:-------------:|:-------------|:-------------:|:-------------|:-------------:|:-------------|\n",
      "| short_sleev.. | 12556        | long_sleeve.. | 5966         | short_sleev.. | 142          |\n",
      "| long_sleeve.. | 2011         |     vest      | 2113         |     sling     | 322          |\n",
      "|    shorts     | 4167         |   trousers    | 9586         |     skirt     | 6522         |\n",
      "| short_sleev.. | 3127         | long_sleeve.. | 1477         |  vest_dress   | 3352         |\n",
      "|  sling_dress  | 1149         |               |              |               |              |\n",
      "|     total     | 52490        |               |              |               |              |\u001b[0m\n",
      "\u001b[32m[06/03 05:19:58 d2.data.common]: \u001b[0mSerializing 32153 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[06/03 05:19:59 d2.data.common]: \u001b[0mSerialized dataset takes 490.68 MiB\n",
      "\u001b[32m[06/03 05:19:59 d2.data.detection_utils]: \u001b[0mTransformGens used in training: [ResizeShortestEdge(short_edge_length=(640, 672, 704, 736, 768, 800), max_size=1333, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[06/03 05:19:59 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.weight' to the model due to incompatible shapes: (81, 1024) in the checkpoint but (14, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.cls_score.bias' to the model due to incompatible shapes: (81,) in the checkpoint but (14,) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.weight' to the model due to incompatible shapes: (320, 1024) in the checkpoint but (52, 1024) in the model! You might want to double check if this is expected.\n",
      "Skip loading parameter 'roi_heads.box_predictor.bbox_pred.bias' to the model due to incompatible shapes: (320,) in the checkpoint but (52,) in the model! You might want to double check if this is expected.\n",
      "Some model parameters or buffers are not found in the checkpoint:\n",
      "\u001b[34mroi_heads.box_predictor.bbox_pred.{bias, weight}\u001b[0m\n",
      "\u001b[34mroi_heads.box_predictor.cls_score.{bias, weight}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True) # create directory output_dir\n",
    "trainer = DefaultTrainer(cfg) \n",
    "trainer.resume_or_load(resume=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"/home/hadil/final/model_final.pth\")  #trained model path\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.55   # set the testing threshold for this model\n",
    "                                               # seuil utilisé pour filtrer low-scored bounding boxes predicted by the Fast R-CNN during the test\n",
    "cfg.DATASETS.TEST = (\"deepfashion_val\", )\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/hadil/final/code02.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadil/final/code02.ipynb#ch0000010?line=0'>1</a>\u001b[0m im \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39m\"\u001b[39m\u001b[39m/home/hadil/final/anchor/sss.jpg\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hadil/final/code02.ipynb#ch0000010?line=1'>2</a>\u001b[0m outputs \u001b[39m=\u001b[39m predictor(im) \u001b[39m#output is a dictionary\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadil/final/code02.ipynb#ch0000010?line=2'>3</a>\u001b[0m \u001b[39m#Metadata is a key-value used to interpret what’s in the dataset( names of classes)..\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadil/final/code02.ipynb#ch0000010?line=3'>4</a>\u001b[0m v \u001b[39m=\u001b[39m Visualizer(im[:, :, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], MetadataCatalog\u001b[39m.\u001b[39mget(cfg\u001b[39m.\u001b[39mDATASETS\u001b[39m.\u001b[39mTRAIN[\u001b[39m0\u001b[39m]), scale\u001b[39m=\u001b[39m\u001b[39m1.2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py:213\u001b[0m, in \u001b[0;36mDefaultPredictor.__call__\u001b[0;34m(self, original_image)\u001b[0m\n\u001b[1;32m    <a href='file:///home/hadil/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py?line=209'>210</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_format \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRGB\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    <a href='file:///home/hadil/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py?line=210'>211</a>\u001b[0m     \u001b[39m# whether the model expects BGR inputs or RGB\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/hadil/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py?line=211'>212</a>\u001b[0m     original_image \u001b[39m=\u001b[39m original_image[:, :, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m--> <a href='file:///home/hadil/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py?line=212'>213</a>\u001b[0m height, width \u001b[39m=\u001b[39m original_image\u001b[39m.\u001b[39;49mshape[:\u001b[39m2\u001b[39m]\n\u001b[1;32m    <a href='file:///home/hadil/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py?line=213'>214</a>\u001b[0m image \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_gen\u001b[39m.\u001b[39mget_transform(original_image)\u001b[39m.\u001b[39mapply_image(original_image)\n\u001b[1;32m    <a href='file:///home/hadil/.local/lib/python3.8/site-packages/detectron2/engine/defaults.py?line=214'>215</a>\u001b[0m image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(image\u001b[39m.\u001b[39mastype(\u001b[39m\"\u001b[39m\u001b[39mfloat32\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "im = cv2.imread(\"/home/hadil/final/anchor/sss.jpg\")\n",
    "outputs = predictor(im) #output is a dictionary\n",
    "#Metadata is a key-value used to interpret what’s in the dataset( names of classes)..\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))  #moving outputs[\"instances\"] to cpu then apply draw_instance_predictions\n",
    "cv2.imshow('imgg',v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "import uuid #helps in generating random objects of 128 bits as ids\n",
    "\n",
    "boxes = {}\n",
    "for coordinates in outputs[\"instances\"].to(\"cpu\").pred_boxes:\n",
    "  coordinates_array = []\n",
    "  for k in coordinates:\n",
    "    coordinates_array.append(int(k))\n",
    "  \n",
    "  boxes[uuid.uuid4().hex[:].upper()] = coordinates_array\n",
    "for k,v in boxes.items():\n",
    "  crop_img = im[v[1]:v[3], v[0]:v[2], :]\n",
    "  cv2.imshow('imgg_class.jpg',crop_img)\n",
    "  cv2.imwrite('img_class.jpg', crop_img)\n",
    "  cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-03 05:20:03.041209: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model = ResNet50(weights='imagenet',include_top=False,input_shape=(224,224,3))\n",
    "model.trainable = False\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    model,\n",
    "    GlobalMaxPooling2D()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['binary_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 2048)             0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Displaying Entire Table product \n",
      "/home/hadil/final/anchor/01.jpeg\n",
      "/home/hadil/final/anchor/02.jpg\n",
      "/home/hadil/final/anchor/03.jpg\n",
      "/home/hadil/final/anchor/04.jpg\n",
      "/home/hadil/final/anchor/05.jpg\n",
      "/home/hadil/final/anchor/06.jpeg\n",
      "/home/hadil/final/anchor/07.jpg\n",
      "/home/hadil/final/anchor/08.jpg\n",
      "/home/hadil/final/anchor/004.jpg\n",
      "/home/hadil/final/anchor/0044.jpg\n",
      "/home/hadil/final/anchor/09.jpeg\n",
      "/home/hadil/final/anchor/20.jpeg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1/12 [00:00<00:02,  5.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 98ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2/12 [00:00<00:01,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 94ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 3/12 [00:00<00:01,  6.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 99ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 4/12 [00:00<00:01,  6.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 95ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5/12 [00:00<00:01,  6.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 93ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 6/12 [00:00<00:00,  6.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 90ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 7/12 [00:01<00:00,  7.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 91ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 8/12 [00:01<00:00,  7.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 96ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 9/12 [00:01<00:00,  7.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 10/12 [00:01<00:00,  7.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 88ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 11/12 [00:01<00:00,  7.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 91ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  7.15it/s]\n"
     ]
    }
   ],
   "source": [
    "filenames=[]\n",
    "\n",
    "print(\"\\nDisplaying Entire Table product \")\n",
    "cursor.execute(\"SELECT * FROM attachment;\")\n",
    "output=cursor.fetchall()\n",
    "for item in output:\n",
    "    print(item[1])\n",
    "    filenames.append(item[1])\n",
    "def extract_features(img_path,model):\n",
    "    img = image.load_img(img_path,target_size=(224,224)) #charger une img\n",
    "    img_array = image.img_to_array(img) #convertir en array\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0) #(1,224,224,3)\n",
    "    preprocessed_img = preprocess_input(expanded_img_array) #The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset\n",
    "    result = model.predict(preprocessed_img).flatten()\n",
    "    normalized_result = result / norm(result)\n",
    "\n",
    "    return normalized_result\n",
    "feature_list = []\n",
    "\n",
    "for file in tqdm(filenames):\n",
    "    feature_list.append(extract_features(file,model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nprint(os.listdir('/home/hadil/final/anchor'))\\nfilenames = []\\n\\nfor file in os.listdir('/home/hadil/final/anchor'):\\n    filenames.append(os.path.join('/home/hadil/final/anchor',file))\\nprint(filenames[0:5])\\ndef extract_features(img_path,model):\\n    img = image.load_img(img_path,target_size=(224,224)) #charger une img\\n    img_array = image.img_to_array(img) #convertir en array\\n    expanded_img_array = np.expand_dims(img_array, axis=0) #(1,224,224,3)\\n    preprocessed_img = preprocess_input(expanded_img_array) #The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset\\n    result = model.predict(preprocessed_img).flatten()\\n    normalized_result = result / norm(result)\\n\\n    return normalized_result\\nfeature_list = []\\n\\nfor file in tqdm(filenames):\\n    feature_list.append(extract_features(file,model))\\n    \""
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "print(os.listdir('/home/hadil/final/anchor'))\n",
    "filenames = []\n",
    "\n",
    "for file in os.listdir('/home/hadil/final/anchor'):\n",
    "    filenames.append(os.path.join('/home/hadil/final/anchor',file))\n",
    "print(filenames[0:5])\n",
    "def extract_features(img_path,model):\n",
    "    img = image.load_img(img_path,target_size=(224,224)) #charger une img\n",
    "    img_array = image.img_to_array(img) #convertir en array\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0) #(1,224,224,3)\n",
    "    preprocessed_img = preprocess_input(expanded_img_array) #The images are converted from RGB to BGR, then each color channel is zero-centered with respect to the ImageNet dataset\n",
    "    result = model.predict(preprocessed_img).flatten()\n",
    "    normalized_result = result / norm(result)\n",
    "\n",
    "    return normalized_result\n",
    "feature_list = []\n",
    "\n",
    "for file in tqdm(filenames):\n",
    "    feature_list.append(extract_features(file,model))\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['IMG_20220408_085855.jpg', 'pull_monde.jpg', 'hmgoepprod (13).jpg', 'Copie de hmgoepprod (31).jpg', 'hmgoepprod (1).jpg', 'Copie de blanc2.jpeg', 'IMG_20220408_085912.jpg', 'IMG_20220408_085731.jpg', 'Copie de bleu clair1.jpeg', 'Copie de berbey1.jpeg', 'Copie de hmgoepprod.jpg', 'IMG_20220408_085743.jpg', 'hmgoepprod (1).jpeg', 'hmgoepprod (17).jpg', 'IMG_20220408_085903.jpg', 'Copie de beige2.jpeg', 'Copie de blanc twil1.jpeg_', 'Copie de hmgoepprod (32).jpg', 'hmgoepprod (31).jpg', 'short.jpeg', 'hmgoepprod (29).jpg', 'hmgoepprod (12).jpg', 'Copie de hmgoepprod (23).jpg', 'Copie de rose1.jpeg', 'IMG_20220408_085827.jpg', 'hmgoepprod.jpg', 'Copie de orange.jpeg', 'hmgoepprod (4).jpg', 'IMG_20220408_085818.jpg', 'Copie de hmgoepprod (24).jpg', 'hmgoepprod (3).jpg', 'hmgoepprod (6).jpg', 'Copie de beige1.jpeg', 'IMG_20220408_085936.jpg', 'orange1.jpeg', 'Copie de berbery.jpeg', 'hmgoepprod (19).jpg', 'hmgoepprod (7).jpg', 'Copie de hmgoepprod (30).jpg', 'hmgoepprod (8).jpg', 'Copie de white.jpeg', 'short_kid.jpeg', 'Copie de mzarkech.jpeg', 'hmgoepprod (2).jpg', 'hmgoepprod (5).jpg', 'chemise_kid.jpeg', 'hmgoepprod (20).jpg', 'Copie de cachemir.jpeg', 'hmgoepprod (11).jpg', 'Copie de hmgoepprod (26).jpg', 'Copie de blanc twil.jpeg', 'hmgoepprod (16).jpg', 'Copie de blanc1.jpeg', 'IMG_20220408_085838.jpg', 'Copie de rose.jpeg', 'IMG_20220408_085847.jpg', 'Copie de hmgoepprod (22).jpg', 'IMG_20220408_085756.jpg', 'Copie de perfecto.jpeg', 'Copie de hmgoepprod (29).jpg', 'hmgoepprod (15).jpg', 'hmgoepprod (14).jpg', 'Copie de mzarkech1.jpeg', 'Copie de rose fless.jpeg', 'hmgoepprod (18).jpg', 'hmgoepprod (32).jpg', 'hmgoepprod (7).jpeg', 'Copie de hmgoepprod (28).jpg', 'Copie de hmgoepprod (25).jpg', 'hmgoepprod (9).jpg', 'Copie de hmgoepprod (21).jpg', 'hmgoepprod (30).jpg', 'Copie de vert.jpeg', 'hmgoepprod (10).jpg']\n"
     ]
    }
   ],
   "source": [
    "#print(os.listdir('/home/hadil/final/anchor'))\n",
    "#filenames = []\n",
    "\n",
    "#for file in os.listdir('/home/hadil/final/anchor'):\n",
    " #   filenames.append(os.path.join('/home/hadil/final/anchor',file))\n",
    "#print(filenames[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(feature_list,open('embedding.pkl','wb'))\n",
    "pickle.dump(filenames,open('filenames.pkl','wb'))\n",
    "feature_list=pickle.load(open('embedding.pkl','rb')) #load features list\n",
    "filenames=pickle.load(open('filenames.pkl','rb')) #load image's paths list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step\n",
      "[[ 8  3 11]]\n",
      "[[4.4703484e-08 6.4920336e-01 6.5273458e-01]]\n",
      "/home/hadil/final/anchor/004.jpg\n",
      "/home/hadil/final/anchor/04.jpg\n",
      "/home/hadil/final/anchor/20.jpeg\n"
     ]
    }
   ],
   "source": [
    "img = image.load_img('/home/hadil/final/anchor/004.jpg',target_size=(224,224)) #charger une img\n",
    "img_array = image.img_to_array(img) #convertir en array\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0) #(1,224,224,3)\n",
    "preprocessed_img = preprocess_input(expanded_img_array)\n",
    "result = model.predict(preprocessed_img).flatten()\n",
    "normalized_result = result / norm(result)\n",
    "neighbors= NearestNeighbors(n_neighbors=3,algorithm='brute', metric='euclidean') #Number of neighbors to use=5, ‘brute’ will use a brute-force search, euclidean: distance entre 2 points\n",
    "neighbors.fit(feature_list)\n",
    "distances,indices= neighbors.kneighbors([normalized_result])\n",
    "print(indices)\n",
    "print(distances)\n",
    "for file in indices[0]:\n",
    "  print(filenames[file])\n",
    "for file in indices[0]:\n",
    "  similar=cv2.imread(filenames[file])\n",
    "  cv2.imshow('sim',similar)\n",
    "  cv2.waitKey(0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = cv2.imread(\"/home/hadil/final/anchor/hmgoepprod (7).jpg\")\n",
    "outputs = predictor(im) #output is a dictionary\n",
    "#Metadata is a key-value used to interpret what’s in the dataset( names of classes)..\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))  #moving outputs[\"instances\"] to cpu then apply draw_instance_predictions\n",
    "cv2.imshow('imgg',v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "im = cv2.imread(\"/home/hadil/final/anchor/hmgoepprod (7).jpg\")\n",
    "outputs = predictor(im) #output is a dictionary\n",
    "#Metadata is a key-value used to interpret what’s in the dataset( names of classes)..\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))  #moving outputs[\"instances\"] to cpu then apply draw_instance_predictions\n",
    "cv2.imshow('imgg',v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "import uuid #helps in generating random objects of 128 bits as ids\n",
    "\n",
    "boxes = {}\n",
    "for coordinates in outputs[\"instances\"].to(\"cpu\").pred_boxes:\n",
    "  coordinates_array = []\n",
    "  for k in coordinates:\n",
    "    coordinates_array.append(int(k))\n",
    "\n",
    "\n",
    "  if(len(coordinates_array)==0):\n",
    "    img = image.load_img('/home/hadil/final/anchor/hmgoepprod (7).jpg',target_size=(224,224)) #charger une img\n",
    "    img_array = image.img_to_array(img) #convertir en array\n",
    "    expanded_img_array = np.expand_dims(img_array, axis=0) #(1,224,224,3)\n",
    "    preprocessed_img = preprocess_input(expanded_img_array)\n",
    "    result = model.predict(preprocessed_img).flatten()\n",
    "    normalized_result = result / norm(result)\n",
    "    neighbors= NearestNeighbors(n_neighbors=3,algorithm='brute', metric='euclidean') #Number of neighbors to use=5, ‘brute’ will use a brute-force search, euclidean: distance entre 2 points\n",
    "    neighbors.fit(feature_list)\n",
    "    distances,indices= neighbors.kneighbors([normalized_result])\n",
    "    print(indices)\n",
    "    print(distances)\n",
    "    for file in indices[0]:\n",
    "      print(filenames[file])\n",
    "    for file in indices[0]:\n",
    "      similar=cv2.imread(filenames[file])\n",
    "      cv2.imshow('sim',similar)\n",
    "      cv2.waitKey(0)\n",
    "      \n",
    "  else: \n",
    "      boxes[uuid.uuid4().hex[:].upper()] = coordinates_array\n",
    "      for k,v in boxes.items():\n",
    "         crop_img = im[v[1]:v[3], v[0]:v[2], :]\n",
    "  \n",
    "         cv2.imshow('part1.jpg',crop_img)\n",
    "         cv2.imwrite('part1.jpg', crop_img)\n",
    "         cv2.waitKey(0) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toutes les instances détéctées\n",
    "im = cv2.imread(\"/home/hadil/final/anchor/04.jpg\")\n",
    "outputs = predictor(im) #output is a dictionary\n",
    "#Metadata is a key-value used to interpret what’s in the dataset( names of classes)..\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))  #moving outputs[\"instances\"] to cpu then apply draw_instance_predictions\n",
    "cv2.imshow('imgg',v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "\n",
    "import uuid #helps in generating random objects of 128 bits as ids\n",
    "\n",
    "boxes = {} \n",
    "for coordinates in outputs[\"instances\"].to(\"cpu\").pred_boxes:\n",
    "  coordinates_array = [] #les coordonnées de chaque instance\n",
    "  for k in coordinates:\n",
    "    coordinates_array.append(int(k))\n",
    "  boxes[uuid.uuid4().hex[:].upper()] = coordinates_array  #les instance lkol fi dict\n",
    "#crop predicted instances\n",
    "for k,v in boxes.items():\n",
    "  crop_img = im[v[1]:v[3], v[0]:v[2], :]\n",
    "  \n",
    "  cv2.imshow('part1.jpg',crop_img)\n",
    "  cv2.imwrite('part1.jpg', crop_img)\n",
    "  cv2.waitKey(0)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 137ms/step\n",
      "[[ 3  8 11]]\n",
      "[[0.646863   0.71235025 0.7554165 ]]\n",
      "/home/hadil/final/anchor/04.jpg\n",
      "/home/hadil/final/anchor/004.jpg\n",
      "/home/hadil/final/anchor/20.jpeg\n"
     ]
    }
   ],
   "source": [
    "######## Instance 0\n",
    "im = cv2.imread(\"/home/hadil/final/anchor/04.jpg\")\n",
    "outputs = predictor(im) #output is a dictionary contient les instances \n",
    "#detect articles\n",
    "v = Visualizer(im[:, :, ::-1], MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2)\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))  #moving outputs[\"instances\"] to cpu then apply draw_instance_predictions\n",
    "cv2.imshow('imgg',v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "import uuid #helps in generating random objects of 128 bits as ids\n",
    "\n",
    "boxes = {}\n",
    "for coordinates in outputs[\"instances\"][0].to(\"cpu\").pred_boxes:\n",
    "  coordinates_array = []\n",
    "  for k in coordinates:\n",
    "    coordinates_array.append(int(k))\n",
    "  \n",
    "  boxes[uuid.uuid4().hex[:].upper()] = coordinates_array\n",
    "for k,v in boxes.items():\n",
    "  crop_img = im[v[1]:v[3], v[0]:v[2], :]\n",
    "  \n",
    "  cv2.imshow('part1.jpg',crop_img)\n",
    "  cv2.imwrite('part1.jpg', crop_img)\n",
    "  cv2.waitKey(0)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "img = image.load_img('part1.jpg',target_size=(224,224)) #charger une img\n",
    "img_array = image.img_to_array(img) #convertir en array\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0) #(1,224,224,3)\n",
    "preprocessed_img = preprocess_input(expanded_img_array)\n",
    "result = model.predict(preprocessed_img).flatten()\n",
    "normalized_result = result / norm(result)\n",
    "neighbors= NearestNeighbors(n_neighbors=3,algorithm='brute', metric='euclidean') #Number of neighbors to use=5, ‘brute’ will use a brute-force search, euclidean: distance entre 2 points\n",
    "neighbors.fit(feature_list)\n",
    "distances,indices= neighbors.kneighbors([normalized_result])\n",
    "print(indices)\n",
    "print(distances)\n",
    "for file in indices[0]:\n",
    "  print(filenames[file])\n",
    "for file in indices[0]:\n",
    "  similar=cv2.imread(filenames[file])\n",
    "  cv2.imshow('sim',similar)\n",
    "  cv2.waitKey(0)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 97ms/step\n",
      "[[ 4  6 10]]\n",
      "[[0.5405096  0.6975931  0.69925404]]\n",
      "/home/hadil/final/anchor/05.jpg\n",
      "/home/hadil/final/anchor/07.jpg\n",
      "/home/hadil/final/anchor/09.jpeg\n"
     ]
    }
   ],
   "source": [
    "#instance1\n",
    "import uuid #helps in generating random objects of 128 bits as ids\n",
    "\n",
    "boxes = {}\n",
    "for coordinates in outputs[\"instances\"][1].to(\"cpu\").pred_boxes:\n",
    "  coordinates_array = []\n",
    "  for k in coordinates:\n",
    "    coordinates_array.append(int(k))\n",
    "  \n",
    "  boxes[uuid.uuid4().hex[:].upper()] = coordinates_array\n",
    "for k,v in boxes.items():\n",
    "  crop_img = im[v[1]:v[3], v[0]:v[2], :]\n",
    "  \n",
    "  cv2.imshow('part2.jpg',crop_img)\n",
    "  cv2.imwrite('part2.jpg', crop_img)\n",
    "  cv2.waitKey(0)\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "img = image.load_img('part2.jpg',target_size=(224,224)) #charger une img\n",
    "img_array = image.img_to_array(img) #convertir en array\n",
    "expanded_img_array = np.expand_dims(img_array, axis=0) #(1,224,224,3)\n",
    "preprocessed_img = preprocess_input(expanded_img_array)\n",
    "result = model.predict(preprocessed_img).flatten()\n",
    "normalized_result = result / norm(result)\n",
    "neighbors= NearestNeighbors(n_neighbors=3,algorithm='brute', metric='euclidean') #Number of neighbors to use=5, ‘brute’ will use a brute-force search, euclidean: distance entre 2 points\n",
    "neighbors.fit(feature_list)\n",
    "distances,indices= neighbors.kneighbors([normalized_result])\n",
    "print(indices)\n",
    "print(distances)\n",
    "for file in indices[0]:\n",
    "  print(filenames[file])\n",
    "for file in indices[0]:\n",
    "  similar=cv2.imread(filenames[file])\n",
    "  cv2.imshow('sim',similar)\n",
    "  cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
