{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#remove skin from product images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "cvpfKpluIfCd"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import load_model\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "from tensorflow.compat.v1 import ConfigProto\n",
        "from tensorflow.compat.v1 import InteractiveSession\n",
        "\n",
        "config = ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "session = InteractiveSession(config=config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "18OWOB_fQURg"
      },
      "outputs": [],
      "source": [
        "########  https://github.com/anish9/Fashion-AI-segmentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "j1rg-CyOIuRz"
      },
      "outputs": [],
      "source": [
        "f = sys.argv[1]\n",
        "saved = load_model('/home/hadil/final/save_ckp_frozen.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-18 06:12:55.120006: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 3s 3s/step\n"
          ]
        }
      ],
      "source": [
        "with tf.device('/cpu:0'):\n",
        "    saved = load_model('/home/hadil/final/save_ckp_frozen.h5')\n",
        "    class fashion_tools(object):\n",
        "     def __init__(self,imageid,model,version=1.1):\n",
        "        self.imageid = imageid\n",
        "        self.model   = model\n",
        "        self.version = version\n",
        "        \n",
        "     def get_dress(self,stack=False):\n",
        "        \"\"\"limited to top wear and full body dresses (wild and studio working)\"\"\"\n",
        "        \"\"\"takes input rgb----> return PNG\"\"\"\n",
        "        name =  self.imageid\n",
        "        file = cv2.imread(\"/home/hadil/final/hmgoepprod.jpeg\")\n",
        "        file = tf.image.resize_with_pad(file,target_height=512,target_width=512)  #Redimensionne et remplit une image à une largeur et une hauteur cibles.\n",
        "        rgb  = file.numpy()\n",
        "        file = np.expand_dims(file,axis=0)/ 255\n",
        "        seq = self.model.predict(file)\n",
        "        seq = seq[3][0,:,:,0]\n",
        "        seq = np.expand_dims(seq,axis=-1)\n",
        "        c1x = rgb*seq\n",
        "        c2x = rgb*(1-seq)\n",
        "        \n",
        "        cfx = c1x+c2x\n",
        "        dummy = np.ones((rgb.shape[0],rgb.shape[1],1))\n",
        "        #rgbx = np.concatenate((rgb,dummy*255),axis=-1)\n",
        "        rgbs = np.concatenate((cfx,seq*255.),axis=-1)\n",
        "        if stack:\n",
        "            #stacked = np.hstack((rgbs,rgbs))\n",
        "            stacked=rgbs\n",
        "            return stacked\n",
        "        else:\n",
        "            return rgbs\n",
        "        \n",
        "        \n",
        "     def get_patch(self):\n",
        "        return None\n",
        "    api    = fashion_tools(f,saved)\n",
        "    image_ = api.get_dress(stack=True)    \n",
        "    cv2.imshow('img',image_)\n",
        "    cv2.waitKey(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f85b006b280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "with tf.device('/cpu:0'):\n",
        "    saved = tf.keras.models.load_model('/home/hadil/final/save_ckp_frozen.h5', compile=False)\n",
        "    class fashion_tools(object):\n",
        "     def __init__(self,imageid,model,version=1.1):\n",
        "        self.imageid = imageid\n",
        "        self.model   = model\n",
        "        self.version = version\n",
        "        \n",
        "     def get_dress(self,stack=False):\n",
        "        #limited to top wear and full body dresses (wild and studio working)\n",
        "        #takes input rgb----> return PNG\n",
        "        name =  self.imageid\n",
        "        file = cv2.imread(\"/home/hadil/final/hmgoepprod.jpeg\")\n",
        "        file = tf.image.resize_with_pad(file,target_height=512,target_width=512)  #Redimensionne et remplit une image à une largeur et une hauteur cibles.\n",
        "        rgb  = file.numpy()\n",
        "        file = np.expand_dims(file,axis=0)/ 255\n",
        "        seq = self.model.predict(file)\n",
        "        seq = seq[3][0,:,:,0]\n",
        "        seq = np.expand_dims(seq,axis=-1)\n",
        "        c1x = rgb*seq\n",
        "        c2x = rgb*(1-seq)\n",
        "        \n",
        "        cfx = c1x+c2x\n",
        "        dummy = np.ones((rgb.shape[0],rgb.shape[1],1))\n",
        "        #rgbx = np.concatenate((rgb,dummy*255),axis=-1)\n",
        "        rgbs = np.concatenate((cfx,seq*255.),axis=-1)\n",
        "        if stack:\n",
        "            #stacked = np.hstack((rgbs,rgbs))\n",
        "            stacked=rgbs\n",
        "            return stacked\n",
        "        else:\n",
        "            return rgbs\n",
        "        \n",
        "        \n",
        "     def get_patch(self):\n",
        "        return None\n",
        "    api    = fashion_tools(f,saved)\n",
        "    image_ = api.get_dress(stack=True)    \n",
        "    cv2.imshow('img',image_)\n",
        "    cv2.waitKey(0)\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "x4-zUxIlI6Yg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2022-05-16 13:38:26.632737: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
            "2022-05-16 13:38:26.632771: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2022-05-16 13:38:26.632797: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (hadil-Inspiron-5570): /proc/driver/nvidia/version does not exist\n",
            "2022-05-16 13:38:26.633274: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2022-05-16 13:38:27.651492: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 18874368 exceeds 10% of free system memory.\n",
            "2022-05-16 13:38:27.657967: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 18874368 exceeds 10% of free system memory.\n",
            "2022-05-16 13:38:27.660372: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 18874368 exceeds 10% of free system memory.\n",
            "2022-05-16 13:38:27.719217: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 18874368 exceeds 10% of free system memory.\n",
            "2022-05-16 13:38:27.728256: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 18874368 exceeds 10% of free system memory.\n"
          ]
        }
      ],
      "source": [
        "saved = load_model(\"/home/hadil/final/save_ckp_frozen.h5\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "with tf.device('/cpu:0'):\n",
        "  saved = tf.keras.models.load_model('/home/hadil/final/save_ckp_frozen.h5', compile=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "czo9_o8kI8q8"
      },
      "outputs": [],
      "source": [
        "class fashion_tools(object):\n",
        "    def __init__(self,imageid,model,version=1.1):\n",
        "        self.imageid = imageid\n",
        "        self.model   = model\n",
        "        self.version = version\n",
        "        \n",
        "    def get_dress(self,stack=False):\n",
        "        \"\"\"limited to top wear and full body dresses (wild and studio working)\"\"\"\n",
        "        \"\"\"takes input rgb----> return PNG\"\"\"\n",
        "        name =  self.imageid\n",
        "        file = cv2.imread(\"/home/hadil/final/hmgoepprod.jpeg\")\n",
        "        file = tf.image.resize_with_pad(file,target_height=512,target_width=512)  #Redimensionne et remplit une image à une largeur et une hauteur cibles.\n",
        "        rgb  = file.numpy()\n",
        "        file = np.expand_dims(file,axis=0)/ 255\n",
        "        seq = self.model.predict(file)\n",
        "        seq = seq[3][0,:,:,0]\n",
        "        seq = np.expand_dims(seq,axis=-1)\n",
        "        c1x = rgb*seq\n",
        "        c2x = rgb*(1-seq)\n",
        "        \n",
        "        cfx = c1x+c2x\n",
        "        dummy = np.ones((rgb.shape[0],rgb.shape[1],1))\n",
        "        rgbx = np.concatenate((rgb,dummy*255),axis=-1)\n",
        "        rgbs = np.concatenate((cfx,seq*255.),axis=-1)\n",
        "        if stack:\n",
        "            stacked = np.hstack((rgbs,rgbs))\n",
        "            stacked=rgbs\n",
        "            return stacked\n",
        "        else:\n",
        "            return rgbs\n",
        "        \n",
        "        \n",
        "    def get_patch(self):\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sxfoGncwJU_-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 2s 2s/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "api    = fashion_tools(f,saved)\n",
        "image_ = api.get_dress(stack=True)\n",
        "\n",
        "cv2.imwrite('xc.jpg',image_)\n",
        "cv2.waitKey(0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "1SSONdMGNYol",
        "outputId": "8c921edc-d70a-4d7c-ff5e-be245098854b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "cv2.imshow('img',image_)\n",
        "cv2.waitKey(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NB3qLxZ8FamF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "remove skin.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
